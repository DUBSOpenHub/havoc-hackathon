# ğŸ§ª Testing Guide

This document describes how to verify the Havoc Hackathon skill works correctly.

Since this is a conversational AI skill (not traditional code), testing is done through **conversation playbooks**  -  scripted interactions that verify expected behavior.

---

## ğŸ® How to Test Locally

1. **Register the skill** in a Copilot CLI session:
   ```
   /skills add ./
   ```

2. **Run each playbook** below and verify the expected behavior.

3. **Check the QA checklist** at the bottom before submitting a PR.

---

## ğŸ“‹ Conversation Playbooks

### Playbook 1: Basic Code Hackathon

| Step | You Say | Expected Behavior |
|------|---------|-------------------|
| 1 | `run hackathon  -  write a fizzbuzz function` | Opening ceremony with arena banner, contestants, rubric |
| 2 | *(accept defaults or customize)* | 3 models dispatched in parallel with progress commentary |
| 3 | *(wait for completion)* | All models finish, outputs normalized |
| 4 | *(judging phase)* | Sealed panel scores anonymized submissions |
| 5 | *(results)* | Drumroll â†’ winner reveal â†’ ASCII podium â†’ ELO update |

### Playbook 2: Review Mode

| Step | You Say | Expected Behavior |
|------|---------|-------------------|
| 1 | `run hackathon  -  review @src/app.js for security issues` | Detects review mode, adjusts rubric |
| 2 | *(models complete)* | Each model produces structured findings |
| 3 | *(results)* | Ensemble report showing consensus findings |

### Playbook 3: Custom Models

| Step | You Say | Expected Behavior |
|------|---------|-------------------|
| 1 | `hackathon with opus and gemini  -  refactor this function` | Only 2 models dispatched (head-to-head mode) |
| 2 | *(results)* | Head-to-head comparison, no bracket/tournament |

### Playbook 4: Model Failure

| Step | You Say | Expected Behavior |
|------|---------|-------------------|
| 1 | `run hackathon` on a complex task | If a model fails, it retries once |
| 2 | *(second failure)* | Model DQ'd with flair ("ğŸ’€ ELIMINATED") |
| 3 | *(remaining models)* | Hackathon continues with surviving contestants |

### Playbook 5: ELO Persistence

| Step | You Say | Expected Behavior |
|------|---------|-------------------|
| 1 | Run hackathon #1 | ELO table created, initial ratings set |
| 2 | Run hackathon #2 | ELO from previous run shown, ratings updated |
| 3 | `show leaderboard` | Current ELO rankings displayed |

### Playbook 6: Smart Merge

| Step | You Say | Expected Behavior |
|------|---------|-------------------|
| 1 | Complete a build hackathon | Merge options presented |
| 2 | Select "Smart merge" | Best components cherry-picked from each submission |
| 3 | *(verify)* | Build passes, tests pass after merge |

### Playbook 7: Audience Participation

| Step | You Say | Expected Behavior |
|------|---------|-------------------|
| 1 | Run a hackathon to completion | After judging, asked "ğŸ™ï¸ Audience vote!" |
| 2 | Rate each submission 1-10 | Scores stored in `hackathon_audience_scores` |
| 3 | *(results)* | Alignment comparison: "You agreed on X but scored Y higher" |

### Playbook 8: Rematch Mode

| Step | You Say | Expected Behavior |
|------|---------|-------------------|
| 1 | Run a hackathon that finishes close (â‰¤2 pts margin) | Offered "ğŸ”¥ Want a rematch with a tiebreaker?" |
| 2 | Accept and pick a 6th criterion | Re-judging on new criterion only |
| 3 | *(results)* | Combined scores reveal final winner |

### Playbook 9: Replay Export

| Step | You Say | Expected Behavior |
|------|---------|-------------------|
| 1 | Complete any hackathon | Offered "ğŸ“¼ Want the highlight reel?" |
| 2 | Accept | Markdown file saved with full transcript |
| 3 | *(verify)* | File contains banner, submissions, scores, podium |

### Playbook 10: Post-Match Analytics

| Step | You Say | Expected Behavior |
|------|---------|-------------------|
| 1 | Run 2+ hackathons in one session | Performance data accumulated |
| 2 | `show stats` or `show leaderboard` | Model trends, win rates, ASCII bar charts |
| 3 | *(verify)* | Per-model breakdown by task type shown |

### Playbook 11: Persistent ELO

| Step | You Say | Expected Behavior |
|------|---------|-------------------|
| 1 | Run a hackathon | ELO saved to `~/.copilot/hackathon-elo.json` |
| 2 | Start a new Copilot CLI session | ELO loaded from JSON file into SQL |
| 3 | Run another hackathon | Previous ELO ratings shown in Phase 0 |

### Playbook 12: Model Tier Selection

| Step | You Say | Expected Behavior |
|------|---------|-------------------|
| 1 | `run hackathon  -  write a haiku` | Prompted: "âš¡ Model tier? Standard or Premium" |
| 2 | Select "Standard" | Standard contestants and judges used, âš¡ badges shown |
| 3 | `run hackathon with premium models  -  write a haiku` | No tier prompt, premium models used directly, ğŸ‘‘ badges shown |
| 4 | `hackathon with opus and gemini  -  write a haiku` | No tier prompt, named models used directly |

---

## âœ… QA Checklist

Before submitting a PR, verify:

- [ ] ğŸ Opening ceremony displays correctly (banner, contestants, rubric)
- [ ] âš¡ Models dispatch in parallel (not sequentially)
- [ ] ğŸ”’ Submissions are anonymized before judging
- [ ] âš–ï¸ 3 judges score independently, median taken
- [ ] ğŸ† Winner reveal has dramatic ceremony (drumroll, podium)
- [ ] ğŸ“ˆ ELO ratings update correctly after each hackathon
- [ ] ğŸ”„ Failed models retry once, then DQ
- [ ] ğŸ§¬ Smart merge produces working code
- [ ] ğŸ­ MC personality is consistent throughout
- [ ] ğŸš¦ Quality gates catch broken builds/tests
- [ ] ğŸ™ï¸ Audience vote prompt appears after judging
- [ ] ğŸ”¥ Rematch offered when margin â‰¤ 2 points
- [ ] ğŸ“¼ Replay export saves valid markdown file
- [ ] ğŸ“Š Post-match analytics display after 2+ hackathons
- [ ] ğŸ’¾ ELO persists to ~/.copilot/hackathon-elo.json
- [ ] âš¡ Tier selection prompt appears when no tier specified
- [ ] ğŸ‘‘ Premium models used when explicitly requested
- [ ] ğŸ·ï¸ Tier badges (âš¡/ğŸ‘‘) shown in opening ceremony

---

## ğŸ” YAML/Markdown Validation

Verify catalog metadata:

```bash
# Check YAML syntax
python3 -c "import yaml; yaml.safe_load(open('skills/havoc-hackathon/catalog.yml'))" && echo "âœ… YAML valid"

# Check required fields
python3 -c "
import yaml
d = yaml.safe_load(open('skills/havoc-hackathon/catalog.yml'))
required = ['schema_version','id','name','description','emoji','codename','category']
missing = [f for f in required if f not in d]
print('âœ… All required fields present' if not missing else f'âŒ Missing: {missing}')
"
```

---

## ğŸ“Š Coverage Matrix

| Feature | Playbook | Status |
|---------|----------|--------|
| Basic code hackathon | 1 | ğŸ§ª |
| Review mode | 2 | ğŸ§ª |
| Custom model selection | 3 | ğŸ§ª |
| Model failure & DQ | 4 | ğŸ§ª |
| ELO persistence | 5 | ğŸ§ª |
| Smart merge | 6 | ğŸ§ª |
| Audience participation | 7 | ğŸ§ª |
| Rematch mode | 8 | ğŸ§ª |
| Replay export | 9 | ğŸ§ª |
| Post-match analytics | 10 | ğŸ§ª |
| Persistent ELO (cross-session) | 11 | ğŸ§ª |
| Model tier selection | 12 | ğŸ§ª |
| Tournament bracket |  -  | ğŸ§ª |
| Adaptive rubrics |  -  | ğŸ§ª |
